{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Evaluation - Groundedness\n",
    "\n",
    "After you have setup and configured the prompt flow, its time to evaluation its performance. Here we can use the prompt flow SDK to test different questions and see how the prompt flow performs using the evaluation prompt flows provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'from promptflow import PFClient' is deprecated and will be removed in the future. Use 'from promptflow.client import PFClient' instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from promptflow import PFClient\n",
    "\n",
    "pf_client = PFClient()\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating connection meta_llama3_instruct...\n",
      "name: meta_llama3_instruct\n",
      "module: promptflow.connections\n",
      "created_date: '2024-05-28T00:27:05.584787'\n",
      "last_modified_date: '2024-06-04T23:45:36.392060'\n",
      "type: serverless\n",
      "api_key: '******'\n",
      "api_base: https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\n",
      "\n",
      "Creating connection gpt2...\n",
      "name: gpt2\n",
      "module: promptflow.connections\n",
      "created_date: '2024-05-28T00:27:05.625707'\n",
      "last_modified_date: '2024-06-04T23:45:36.446080'\n",
      "type: serverless\n",
      "api_key: '******'\n",
      "api_base: https://api-inference.huggingface.co/models/openai-community/gpt2\n",
      "\n",
      "Creating connection bge-small...\n",
      "name: bge-small\n",
      "module: promptflow.connections\n",
      "created_date: '2024-05-28T00:27:05.653943'\n",
      "last_modified_date: '2024-06-04T23:45:36.478673'\n",
      "type: serverless\n",
      "api_key: '******'\n",
      "api_base: https://api-inference.huggingface.co/models/BAAI/bge-small-en\n",
      "\n",
      "Creating connection bge-large...\n",
      "name: bge-large\n",
      "module: promptflow.connections\n",
      "created_date: '2024-05-28T00:27:05.682982'\n",
      "last_modified_date: '2024-06-04T23:45:36.506094'\n",
      "type: serverless\n",
      "api_key: '******'\n",
      "api_base: https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "from promptflow._sdk.entities._connection import ServerlessConnection\n",
    "HF_KEY = \"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\" \n",
    "\n",
    "\n",
    "HF_endpoints = {\"meta_llama3_instruct\":\"https://api-inference.huggingface.co/models/meta-llama/Meta-Llama-3-8B-Instruct\",\"gpt2\":\"https://api-inference.huggingface.co/models/openai-community/gpt2\",\"bge-small\":\"https://api-inference.huggingface.co/models/BAAI/bge-small-en\",\"bge-large\":\"https://api-inference.huggingface.co/models/BAAI/bge-large-en-v1.5\"}#{name:api_base}\n",
    "\n",
    "for name, end_point in HF_endpoints.items():\n",
    "    connection =ServerlessConnection(name=name,api_key=HF_KEY,api_base=end_point)\n",
    "    print(f\"Creating connection {connection.name}...\")\n",
    "    result = pf_client.connections.create_or_update(connection)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pf connection list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'from promptflow import tool' is deprecated and will be removed in the future. Use 'from promptflow.core import tool' instead.\n",
      "[2024-06-04 23:45:41 +0000][promptflow.core._connection][WARNING] - Please use connection.secrets[key] to access secrets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "2024-06-04 23:45:41 +0000    7762 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-06-04 23:45:41 +0000    7762 execution.flow     INFO     Start to run 5 nodes with concurrency level 16.\n",
      "2024-06-04 23:45:41 +0000    7762 execution.flow     INFO     Executing node question_embedding. node run id: c2206879-2ed0-4218-9d7c-ea1b0fedf62d_question_embedding_0\n",
      "2024-06-04 23:45:41 +0000    7762 execution.flow     INFO     Executing node customer_lookup. node run id: c2206879-2ed0-4218-9d7c-ea1b0fedf62d_customer_lookup_0\n",
      "2024-06-04 23:45:41 +0000    7762 execution.flow     INFO     Node customer_lookup completes.\n",
      "2024-06-04 23:45:42 +0000    7762 execution.flow     INFO     Node question_embedding completes.\n",
      "2024-06-04 23:45:42 +0000    7762 execution.flow     INFO     Executing node retrieve_documentation. node run id: c2206879-2ed0-4218-9d7c-ea1b0fedf62d_retrieve_documentation_0\n",
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=contoso-chat&uiTraceId=0xc9b6c5d59ed62a5242b33266c2da2067\n",
      "2024-06-04 23:45:42 +0000    7762 execution.flow     INFO     Node retrieve_documentation completes.\n",
      "2024-06-04 23:45:42 +0000    7762 execution.flow     INFO     Executing node customer_prompt. node run id: c2206879-2ed0-4218-9d7c-ea1b0fedf62d_customer_prompt_0\n",
      "2024-06-04 23:45:42 +0000    7762 execution.flow     INFO     Node customer_prompt completes.\n",
      "2024-06-04 23:45:42 +0000    7762 execution.flow     INFO     Executing node llm_response. node run id: c2206879-2ed0-4218-9d7c-ea1b0fedf62d_llm_response_0\n",
      "2024-06-04 23:45:42 +0000    7762 execution.flow     INFO     Node llm_response completes.\n"
     ]
    }
   ],
   "source": [
    "# Add a question to test the base prompt flow.\n",
    "question = \"Can you tell me about your jackets?\"\n",
    "customerId = \"4\"\n",
    "output = pf_client.test(\n",
    "    flow=\"../contoso-chat\", # Path to the flow directory\n",
    "    inputs={ # Inputs to the flow\n",
    "        \"chat_history\": [],\n",
    "        \"question\": question,\n",
    "        \"customerId\": customerId,\n",
    "    },\n",
    ")\n",
    "\n",
    "output[\"answer\"] = \"\".join(list(output[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': \"Of course, Sarah Lee! üòÑ We have two fantastic jackets that would be perfect for your outdoor adventures:\\n\\n1. Summit Breeze Jacket: üèîÔ∏èüå¨Ô∏è This lightweight jacket is your ultimate companion for hiking. It's windproof, water-resistant, and has adjustable cuffs for your comfort. With its inner lining and reflective accents, you'll feel confident day or night. It's time to reach new heights with the Summit Breeze Jacket! \\n\\n2. RainGuard Hiking Jacket: ‚òî‚õ∞Ô∏è Don't let the weather stop you! This jacket is waterproof, breathable, and has ventilation zippers for increased airflow. With adjustable features and plenty of pockets, it's perfect for all your outdoor undertakings!\\n\\nBoth jackets are amazing options, and with your Platinum membership status, you'll get the best deals! üåü Let me know if you need any more information or recommendations. Happy exploring! üöÄ\",\n",
       " 'context': [{'id': '3',\n",
       "   'title': 'Summit Breeze Jacket',\n",
       "   'content': \"Discover the joy of hiking with MountainStyle's Summit Breeze Jacket. This lightweight jacket is your perfect companion for outdoor adventures. Sporting a trail-ready, windproof design and a water-resistant fabric, it's ready to withstand any weather. The breathable polyester material and adjustable cuffs keep you comfortable, whether you're ascending a mountain or strolling through a park. And its sleek black color adds style to function. The jacket features a full-zip front closure, adjustable hood, and secure zippered pockets. Experience the comfort of its inner lining and the convenience of its packable design. Crafted for night trekkers too, the jacket has reflective accents for enhanced visibility. Rugged yet chic, the Summit Breeze Jacket is more than a hiking essential, it's the gear that inspires you to reach new heights. Choose adventure, choose the Summit Breeze Jacket.\",\n",
       "   'url': '/products/summit-breeze-jacket'},\n",
       "  {'id': '17',\n",
       "   'title': 'RainGuard Hiking Jacket',\n",
       "   'content': \"Introducing the MountainStyle RainGuard Hiking Jacket - the ultimate solution for weatherproof comfort during your outdoor undertakings! Designed with waterproof, breathable fabric, this jacket promises an outdoor experience that's as dry as it is comfortable. The rugged construction assures durability, while the adjustable hood provides a customizable fit against wind and rain. Featuring multiple pockets for safe, convenient storage and adjustable cuffs and hem, you can tailor the jacket to suit your needs on-the-go. And, don't worry about overheating during intense activities - it's equipped with ventilation zippers for increased airflow. Reflective details ensure visibility even during low-light conditions, making it perfect for evening treks. With its lightweight, packable design, carrying it inside your backpack requires minimal effort. With options for men and women, the RainGuard Hiking Jacket is perfect for hiking, camping, trekking and countless other outdoor adventures. Don't let the weather stand in your way - embrace the outdoors with MountainStyle RainGuard Hiking Jacket!\",\n",
       "   'url': '/products/rainguard-hiking-jacket'},\n",
       "  {'id': '4',\n",
       "   'title': 'TrekReady Hiking Boots',\n",
       "   'content': \"Introducing the TrekReady Hiking Boots - stepping up your hiking game, one footprint at a time! Crafted from leather, these stylistic Trailmates are made to last. TrekReady infuses durability with its reinforced stitching and toe protection, making sure your journey is never stopped short. Comfort? They have that covered too! The boots are a haven with their breathable materials, cushioned insole, with padded collar and tongue; all nestled neatly within their lightweight design. As they say, it's what's inside that counts - so inside you'll find a moisture-wicking lining that quarantines stank and keeps your feet fresh as that mountaintop breeze. Remember the fear of slippery surfaces? With these boots, you can finally tell it to 'take a hike'! Their shock-absorbing midsoles and excellent traction capabilities promise stability at your every step. Beautifully finished in a traditional lace-up system, every adventurer deserves a pair of TrekReady Hiking Boots. Hike more, worry less!\",\n",
       "   'url': '/products/trekready-hiking-boots'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-04 23:45:54 +0000][promptflow.core._connection][WARNING] - Please use connection.secrets[key] to access secrets.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt flow service has started...\n",
      "2024-06-04 23:45:55 +0000    7762 execution.flow     INFO     Start executing nodes in thread pool mode.\n",
      "2024-06-04 23:45:55 +0000    7762 execution.flow     INFO     Start to run 5 nodes with concurrency level 16.\n",
      "2024-06-04 23:45:55 +0000    7762 execution.flow     INFO     Executing node question_embedding. node run id: 55e6c9a3-bd8a-4ff2-b693-e2423bfe26b9_question_embedding_0\n",
      "2024-06-04 23:45:55 +0000    7762 execution.flow     INFO     Executing node customer_lookup. node run id: 55e6c9a3-bd8a-4ff2-b693-e2423bfe26b9_customer_lookup_0\n",
      "2024-06-04 23:45:55 +0000    7762 execution.flow     INFO     Node question_embedding completes.\n",
      "2024-06-04 23:45:55 +0000    7762 execution.flow     INFO     Executing node retrieve_documentation. node run id: 55e6c9a3-bd8a-4ff2-b693-e2423bfe26b9_retrieve_documentation_0\n",
      "2024-06-04 23:45:55 +0000    7762 execution.flow     INFO     Node customer_lookup completes.\n",
      "2024-06-04 23:45:56 +0000    7762 execution.flow     INFO     Node retrieve_documentation completes.\n",
      "2024-06-04 23:45:56 +0000    7762 execution.flow     INFO     Executing node customer_prompt. node run id: 55e6c9a3-bd8a-4ff2-b693-e2423bfe26b9_customer_prompt_0\n",
      "2024-06-04 23:45:56 +0000    7762 execution.flow     INFO     Node customer_prompt completes.\n",
      "2024-06-04 23:45:56 +0000    7762 execution.flow     INFO     Executing node llm_response. node run id: 55e6c9a3-bd8a-4ff2-b693-e2423bfe26b9_llm_response_0\n",
      "2024-06-04 23:45:56 +0000    7762 execution.flow     INFO     Node llm_response completes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can view the trace detail from the following URL:\n",
      "http://127.0.0.1:23333/v1.0/ui/traces/?#collection=contoso-chat&uiTraceId=0xa8c17ed8447702ef8b56d1d656e6b936\n"
     ]
    },
    {
     "ename": "APIError",
     "evalue": "An error occurred during streaming",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAPIError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 13\u001b[0m\n\u001b[1;32m      3\u001b[0m customerId \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m output_hf \u001b[38;5;241m=\u001b[39m pf_client\u001b[38;5;241m.\u001b[39mtest(\n\u001b[1;32m      5\u001b[0m     flow\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../contoso-chat_hf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# Path to the flow directory\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     inputs\u001b[38;5;241m=\u001b[39m{ \u001b[38;5;66;03m# Inputs to the flow\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m     },\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m output_hf[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_hf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43manswer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/_trace.py:238\u001b[0m, in \u001b[0;36mtraced_generator\u001b[0;34m(span, inputs, generator, trace_type)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     generator_proxy \u001b[38;5;241m=\u001b[39m GeneratorProxy(generator)\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m generator_proxy\n\u001b[1;32m    240\u001b[0m     generator_output \u001b[38;5;241m=\u001b[39m generator_proxy\u001b[38;5;241m.\u001b[39mitems\n\u001b[1;32m    242\u001b[0m     enrich_span_with_llm_if_needed(span, inputs, generator_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/contracts/generator_proxy.py:19\u001b[0m, in \u001b[0;36mGeneratorProxy.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/contracts/generator_proxy.py:34\u001b[0m, in \u001b[0;36mgenerate_from_proxy\u001b[0;34m(proxy)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_from_proxy\u001b[39m(proxy: GeneratorProxy):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m proxy\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/contracts/generator_proxy.py:19\u001b[0m, in \u001b[0;36mGeneratorProxy.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tools/common.py:472\u001b[0m, in \u001b[0;36mpost_process_chat_api_response.<locals>.generator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerator\u001b[39m():\n\u001b[0;32m--> 472\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mhasattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m                                                    \u001b[49m\u001b[43mchunk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdelta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/_trace.py:238\u001b[0m, in \u001b[0;36mtraced_generator\u001b[0;34m(span, inputs, generator, trace_type)\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    237\u001b[0m     generator_proxy \u001b[38;5;241m=\u001b[39m GeneratorProxy(generator)\n\u001b[0;32m--> 238\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m generator_proxy\n\u001b[1;32m    240\u001b[0m     generator_output \u001b[38;5;241m=\u001b[39m generator_proxy\u001b[38;5;241m.\u001b[39mitems\n\u001b[1;32m    242\u001b[0m     enrich_span_with_llm_if_needed(span, inputs, generator_output)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/contracts/generator_proxy.py:19\u001b[0m, in \u001b[0;36mGeneratorProxy.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/contracts/generator_proxy.py:34\u001b[0m, in \u001b[0;36mgenerate_from_proxy\u001b[0;34m(proxy)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_from_proxy\u001b[39m(proxy: GeneratorProxy):\n\u001b[0;32m---> 34\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m proxy\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/promptflow/tracing/contracts/generator_proxy.py:19\u001b[0m, in \u001b[0;36mGeneratorProxy.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 19\u001b[0m     item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_items\u001b[38;5;241m.\u001b[39mappend(item)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m item\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_streaming.py:43\u001b[0m, in \u001b[0;36mStream.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _T:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/openai/_streaming.py:72\u001b[0m, in \u001b[0;36mStream.__stream__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m message \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(message, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     70\u001b[0m             message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred during streaming\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 72\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m APIError(\n\u001b[1;32m     73\u001b[0m             message\u001b[38;5;241m=\u001b[39mmessage,\n\u001b[1;32m     74\u001b[0m             request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mrequest,\n\u001b[1;32m     75\u001b[0m             body\u001b[38;5;241m=\u001b[39mdata[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     76\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m process_data(data\u001b[38;5;241m=\u001b[39mdata, cast_to\u001b[38;5;241m=\u001b[39mcast_to, response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mAPIError\u001b[0m: An error occurred during streaming"
     ]
    }
   ],
   "source": [
    "# Add a question to test the base prompt flow.\n",
    "question = \"Can you tell me about your jackets?\"\n",
    "customerId = \"4\"\n",
    "output_hf = pf_client.test(\n",
    "    flow=\"../contoso-chat_hf\", # Path to the flow directory\n",
    "    inputs={ # Inputs to the flow\n",
    "        \"chat_history\": [],\n",
    "        \"question\": question,\n",
    "        \"customerId\": customerId,\n",
    "    },\n",
    ")\n",
    "\n",
    "output_hf[\"answer\"] = \"\".join(list(output_hf[\"answer\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the groundedness of the prompt flow with the answer from the above question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pf_client.test(\n",
    "    flow=\"groundedness\",\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "        \"context\": str(output[\"context\"]),\n",
    "        \"answer\": output[\"answer\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hf = pf_client.test(\n",
    "    flow=\"groundedness\",\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "        \"context\": str(output_hf[\"context\"]),\n",
    "        \"answer\": output_hf[\"answer\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Local Evaluation - Multiple Metrics \n",
    "\n",
    "Now use the same prompt flow and test it against the Multi Evaluation flow for groundedness, coherence, fluency, and relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"question\",question)\n",
    "print(\"context\",output[\"context\"])\n",
    "print(\"answer\", output[\"answer\"])\n",
    "test_multi = pf_client.test(\n",
    "    \"multi_flow\",\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "        \"context\": str(output[\"context\"]),\n",
    "        \"answer\": output[\"answer\"],\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"question\",question)\n",
    "print(\"context\",output[\"context\"])\n",
    "print(\"answer\", output[\"answer\"])\n",
    "test_multi_hf = pf_client.test(\n",
    "    \"multi_flow\",\n",
    "    inputs={\n",
    "        \"question\": question,\n",
    "        \"context\": str(output_hf[\"context\"]),\n",
    "        \"answer\": output_hf[\"answer\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_multi_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Studio Azure batch run on an evaluation json dataset\n",
    "\n",
    "Now in order to test these more thoroughly, we can use the Azure AI Studio to run batches of test data with the evaluation prompt flow on a larger dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# Import required libraries\n",
    "from promptflow.azure import PFClient\n",
    "\n",
    "# Import required libraries\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Populate the `config.json` file with the subscription_id, resource_group, and workspace_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found the config file in: ../config.json\n",
      "Found the config file in: ../config.json\n"
     ]
    }
   ],
   "source": [
    "config_path = \"../config.json\"\n",
    "pf_azure_client = PFClient.from_config(credential=credential, path=config_path)\n",
    "\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "    \n",
    "pf_azure_client_hf = PFClient.from_config(credential=credential, path=config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the runtime from the AI Studio that will be used for the cloud batch runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the runtime to the name of the runtime you created previously\n",
    "runtime = \"automatic\"\n",
    "# load flow\n",
    "flow = \"../contoso-chat\"\n",
    "flow_hf = \"../contoso-chat_hf\"\n",
    "# load data\n",
    "data = \"../data/salestestdata.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024_06_04_142628chat_base_run\n"
     ]
    }
   ],
   "source": [
    "# get current time stamp for run name\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp = now.strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "run_name = timestamp+\"chat_base_run\"\n",
    "print(run_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a base run to use as the variant for the evaluation runs. \n",
    "\n",
    "_NOTE: If you get \"'An existing connection was forcibly closed by the remote host'\" run the cell again._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-06-04 14:26:42 +0000][promptflow][WARNING] - You're using compute session, if it's first time you're using it, it may take a while to build session and you may see 'NotStarted' status for a while. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portal url: https://ai.azure.com/projectflows/trace/run/2024_06_04_142628chat_base_run/details?wsid=/subscriptions/28e0cd19-9f05-4b6c-bac8-3bb37e8eeee3/resourcegroups/rg-deployment6/providers/Microsoft.MachineLearningServices/workspaces/contoso-chat-sf-aiproj\n",
      "name: 2024_06_04_142628chat_base_run\n",
      "created_on: '2024-06-04T14:26:48.142237+00:00'\n",
      "status: Preparing\n",
      "display_name: 2024_06_04_142628chat_base_run\n",
      "description:\n",
      "tags: {}\n",
      "properties:\n",
      "  azureml.promptflow.inputs_mapping: '{\"customerId\":\"${data.customerId}\",\"question\":\"${data.question}\"}'\n",
      "  azureml.promptflow.runtime_name: automatic\n",
      "  azureml.promptflow.disable_trace: 'false'\n",
      "  azureml.promptflow.session_id: 3ee7611837014426b3f88afef8d079a99d16385b9b37e347\n",
      "  azureml.promptflow.definition_file_name: flow.dag.yaml\n",
      "  azureml.promptflow.flow_lineage_id: 23ccbb0c520a4d89658b65dfca05e64d37bb66f7118de0ef2e9c2fce1513d985\n",
      "  azureml.promptflow.flow_definition_datastore_name: workspaceblobstore\n",
      "  azureml.promptflow.flow_definition_blob_path: LocalUpload/09c8912aed3d182dd1ca96a56f5e2f86/contoso-chat/flow.dag.yaml\n",
      "  _azureml.evaluation_run: promptflow.BatchRun\n",
      "  azureml.promptflow.snapshot_id: cf914f35-01bb-42fa-9002-cce5f3c04a78\n",
      "  azureml.promptflow.runtime_version: 20240515.v1\n",
      "creation_context:\n",
      "  userObjectId: 43e494fa-d160-4f6b-acc8-5d56e5f2e891\n",
      "  userPuId: 100320037AF139DC\n",
      "  userIdp: https://sts.windows.net/2b897507-ee8c-4575-830b-4f8267c3d307/\n",
      "  userAltSecId: 5::100320011172729C\n",
      "  userIss: https://sts.windows.net/b8c35a17-09c7-40ba-9173-6be750b65674/\n",
      "  userTenantId: b8c35a17-09c7-40ba-9173-6be750b65674\n",
      "  userName: Jim Zhu\n",
      "  upn:\n",
      "start_time:\n",
      "end_time:\n",
      "duration:\n",
      "portal_url: \n",
      "  https://ai.azure.com/projectflows/trace/run/2024_06_04_142628chat_base_run/details?wsid=/subscriptions/28e0cd19-9f05-4b6c-bac8-3bb37e8eeee3/resourcegroups/rg-deployment6/providers/Microsoft.MachineLearningServices/workspaces/contoso-chat-sf-aiproj\n",
      "data: \n",
      "  azureml://datastores/workspaceblobstore/paths/LocalUpload/4d587c1ae8d46401e3a71b74abe367df/salestestdata.jsonl\n",
      "output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create base run in Azure Ai Studio\n",
    "base_run = pf_azure_client.run(\n",
    "    flow=flow,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${data.customerId}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    "    # create a display name as current datetime\n",
    "    display_name=run_name,\n",
    "    name=run_name\n",
    ")\n",
    "print(base_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024_06_04_145016chat_base_run\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mUploading contoso-chat_hf (0.68 MBs): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 681185/681185 [00:00<00:00, 2914046.17it/s]\n",
      "\u001b[39m\n",
      "\n",
      "[2024-06-04 14:50:17 +0000][promptflow][WARNING] - You're using compute session, if it's first time you're using it, it may take a while to build session and you may see 'NotStarted' status for a while. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portal url: https://ai.azure.com/projectflows/trace/run/2024_06_04_145016chat_base_run/details?wsid=/subscriptions/28e0cd19-9f05-4b6c-bac8-3bb37e8eeee3/resourcegroups/rg-deployment6/providers/Microsoft.MachineLearningServices/workspaces/contoso-chat-sf-aiproj\n",
      "name: 2024_06_04_145016chat_base_run\n",
      "created_on: '2024-06-04T14:50:20.522498+00:00'\n",
      "status: Preparing\n",
      "display_name: 2024_06_04_145016chat_base_run\n",
      "description:\n",
      "tags: {}\n",
      "properties:\n",
      "  azureml.promptflow.inputs_mapping: '{\"customerId\":\"${data.customerId}\",\"question\":\"${data.question}\"}'\n",
      "  azureml.promptflow.runtime_name: automatic\n",
      "  azureml.promptflow.disable_trace: 'false'\n",
      "  azureml.promptflow.session_id: 52c804293cb9063afef04d2f62e589db26eec11842742774\n",
      "  azureml.promptflow.definition_file_name: flow.dag.yaml\n",
      "  azureml.promptflow.flow_lineage_id: 48a9e2ae4164ef0535a3b46a60b5817fd940cf9d74fc404427d60512216720df\n",
      "  azureml.promptflow.flow_definition_datastore_name: workspaceblobstore\n",
      "  azureml.promptflow.flow_definition_blob_path: LocalUpload/05a957b610161a6d2337e73e484539c8/contoso-chat_hf/flow.dag.yaml\n",
      "  _azureml.evaluation_run: promptflow.BatchRun\n",
      "  azureml.promptflow.snapshot_id: 8bca850a-8b23-4270-b149-f8a0ecc9b49b\n",
      "  azureml.promptflow.runtime_version: 20240515.v1\n",
      "creation_context:\n",
      "  userObjectId: 43e494fa-d160-4f6b-acc8-5d56e5f2e891\n",
      "  userPuId: 100320037AF139DC\n",
      "  userIdp: https://sts.windows.net/2b897507-ee8c-4575-830b-4f8267c3d307/\n",
      "  userAltSecId: 5::100320011172729C\n",
      "  userIss: https://sts.windows.net/b8c35a17-09c7-40ba-9173-6be750b65674/\n",
      "  userTenantId: b8c35a17-09c7-40ba-9173-6be750b65674\n",
      "  userName: Jim Zhu\n",
      "  upn:\n",
      "start_time:\n",
      "end_time:\n",
      "duration:\n",
      "portal_url: \n",
      "  https://ai.azure.com/projectflows/trace/run/2024_06_04_145016chat_base_run/details?wsid=/subscriptions/28e0cd19-9f05-4b6c-bac8-3bb37e8eeee3/resourcegroups/rg-deployment6/providers/Microsoft.MachineLearningServices/workspaces/contoso-chat-sf-aiproj\n",
      "data: \n",
      "  azureml://datastores/workspaceblobstore/paths/LocalUpload/4d587c1ae8d46401e3a71b74abe367df/salestestdata.jsonl\n",
      "output:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get current time stamp for run name\n",
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "timestamp_hf = now.strftime(\"%Y_%m_%d_%H%M%S\")\n",
    "run_name_hf = timestamp_hf+\"chat_base_run\"\n",
    "print(run_name_hf)\n",
    "\n",
    "# create base run in Azure Ai Studio\n",
    "base_run_hf = pf_azure_client_hf.run(\n",
    "    flow=flow_hf,\n",
    "    data=data,\n",
    "    column_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${data.customerId}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    "    # create a display name as current datetime\n",
    "    display_name=run_name_hf,\n",
    "    name=run_name_hf\n",
    ")\n",
    "print(base_run_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-04 14:50:26 +0000     120 promptflow-runtime INFO     [2024_06_04_145016chat_base_run] Receiving v2 bulk run request da648ef2-eab7-4f7f-8677-ad707184f7c4: {\"flow_id\": \"2024_06_04_145016chat_base_run\", \"flow_run_id\": \"2024_06_04_145016chat_base_run\", \"flow_source\": {\"flow_source_type\": 1, \"flow_source_info\": {\"snapshot_id\": \"8bca850a-8b23-4270-b149-f8a0ecc9b49b\"}, \"flow_dag_file\": \"flow.dag.yaml\"}, \"connections\": \"**data_scrubbed**\", \"log_path\": \"https://stcontosoz2rl7ers7zqcm.blob.core.windows.net/095e81c8-ded8-4405-8381-dd6f75af10c3-azureml/ExperimentRun/dcid.2024_06_04_145016chat_base_run/logs/azureml/executionlogs.txt?sv=2019-07-07&sr=b&sig=**data_scrubbed**&skoid=196af4dc-b9e8-4c71-858c-f1cdd5b8c37d&sktid=b8c35a17-09c7-40ba-9173-6be750b65674&skt=2024-06-04T09%3A25%3A37Z&ske=2024-06-05T17%3A35%3A37Z&sks=b&skv=2019-07-07&st=2024-06-04T14%3A40%3A24Z&se=2024-06-04T22%3A50%3A24Z&sp=rcw\", \"app_insights_instrumentation_key\": \"InstrumentationKey=**data_scrubbed**;IngestionEndpoint=https://switzerlandnorth-0.in.applicationinsights.azure.com/;LiveEndpoint=https://switzerlandnorth.livediagnostics.monitor.azure.com/\", \"flow_name\": \"2024_06_04_145016chat_base_run\", \"batch_timeout_sec\": 36000, \"data_inputs\": {\"data\": \"azureml://datastores/workspaceblobstore/paths/LocalUpload/4d587c1ae8d46401e3a71b74abe367df/salestestdata.jsonl\"}, \"inputs_mapping\": {\"customerId\": \"${data.customerId}\", \"question\": \"${data.question}\"}, \"azure_storage_setting\": {\"azure_storage_mode\": 1, \"storage_account_name\": \"stcontosoz2rl7ers7zqcm\", \"blob_container_name\": \"095e81c8-ded8-4405-8381-dd6f75af10c3-azureml-blobstore\", \"flow_artifacts_root_path\": \"promptflow/PromptFlowArtifacts/2024_06_04_145016chat_base_run\", \"blob_container_sas_token\": \"?sv=2019-07-07&sr=c&sig=**data_scrubbed**&skoid=196af4dc-b9e8-4c71-858c-f1cdd5b8c37d&sktid=b8c35a17-09c7-40ba-9173-6be750b65674&skt=2024-06-04T14%3A50%3A26Z&ske=2024-06-11T14%3A50%3A26Z&sks=b&skv=2019-07-07&se=2024-06-11T14%3A50%3A26Z&sp=racwl\", \"output_datastore_name\": \"workspaceblobstore\"}}\n",
      "2024-06-04 14:50:26 +0000     120 promptflow-runtime INFO     Runtime version: 20240515.v1. PromptFlow version: 1.11.0rc4\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Updating 2024_06_04_145016chat_base_run to Status.Preparing...\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Downloading snapshot to /mnt/host/service/app/43691/requests/2024_06_04_145016chat_base_run\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Get snapshot sas url for 8bca850a-8b23-4270-b149-f8a0ecc9b49b.\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Snapshot 8bca850a-8b23-4270-b149-f8a0ecc9b49b contains 14 files.\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Download snapshot 8bca850a-8b23-4270-b149-f8a0ecc9b49b completed.\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Successfully download snapshot to /mnt/host/service/app/43691/requests/2024_06_04_145016chat_base_run\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     About to execute a python flow.\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Use spawn method to start child process.\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Starting to check process 3470 status for run 2024_06_04_145016chat_base_run\n",
      "2024-06-04 14:50:28 +0000     120 promptflow-runtime INFO     Start checking run status for run 2024_06_04_145016chat_base_run\n",
      "2024-06-04 14:50:33 +0000    3470 promptflow-runtime INFO     [120--3470] Start processing flowV2......\n",
      "2024-06-04 14:50:33 +0000    3470 promptflow-runtime INFO     Runtime version: 20240515.v1. PromptFlow version: 1.11.0rc4\n",
      "2024-06-04 14:50:33 +0000    3470 promptflow-runtime INFO     Setting mlflow tracking uri...\n",
      "2024-06-04 14:50:33 +0000    3470 promptflow-runtime INFO     Validating 'AzureML Data Scientist' user authentication...\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Successfully validated 'AzureML Data Scientist' user authentication.\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Using AzureMLRunStorageV2\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Setting mlflow tracking uri to 'azureml://switzerlandnorth.api.azureml.ms/mlflow/v1.0/subscriptions/28e0cd19-9f05-4b6c-bac8-3bb37e8eeee3/resourceGroups/rg-deployment6/providers/Microsoft.MachineLearningServices/workspaces/contoso-chat-sf-aiproj'\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Setting mlflow tracking uri to 'azureml://switzerlandnorth.api.azureml.ms/mlflow/v1.0/subscriptions/28e0cd19-9f05-4b6c-bac8-3bb37e8eeee3/resourceGroups/rg-deployment6/providers/Microsoft.MachineLearningServices/workspaces/contoso-chat-sf-aiproj'\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Creating unregistered output Asset for Run 2024_06_04_145016chat_base_run...\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Created debug_info Asset: azureml://locations/switzerlandnorth/workspaces/095e81c8-ded8-4405-8381-dd6f75af10c3/data/azureml_2024_06_04_145016chat_base_run_output_data_debug_info/versions/1\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Creating unregistered output Asset for Run 2024_06_04_145016chat_base_run...\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Created flow_outputs output Asset: azureml://locations/switzerlandnorth/workspaces/095e81c8-ded8-4405-8381-dd6f75af10c3/data/azureml_2024_06_04_145016chat_base_run_output_data_flow_outputs/versions/1\n",
      "2024-06-04 14:50:35 +0000    3470 promptflow-runtime INFO     Patching 2024_06_04_145016chat_base_run...\n",
      "2024-06-04 14:50:40 +0000    3470 promptflow-runtime INFO     Resolve data from url finished in 1.9513647640014824 seconds\n",
      "2024-06-04 14:50:40 +0000    3470 promptflow-runtime INFO     Starting the aml run '2024_06_04_145016chat_base_run'...\n",
      "2024-06-04 14:50:42 +0000    3470 execution.bulk     INFO     The timeout for the batch run is 36000 seconds.\n",
      "2024-06-04 14:50:42 +0000    3470 execution.bulk     INFO     Set process count to 4 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 5}.\n",
      "2024-06-04 14:50:48 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:3)-Process id(3647)-Line number(0) start execution.\n",
      "2024-06-04 14:50:48 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:1)-Process id(3627)-Line number(1) start execution.\n",
      "2024-06-04 14:50:48 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:4)-Process id(3656)-Line number(3) start execution.\n",
      "2024-06-04 14:50:48 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:2)-Process id(3637)-Line number(2) start execution.\n",
      "2024-06-04 14:50:51 +0000    3637 execution          WARNING  [llm_response in line 2 (index starts from 0)] stderr> Exception occurs: NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "2024-06-04 14:50:51 +0000    3637 execution          ERROR    Node llm_response in line 2 failed. Exception: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors].\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 543, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/aoai.py\", line 171, in chat\n",
      "    completion = self._client.chat.completions.create(**params)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_integrations/_openai_injector.py\", line 88, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 90, in invoke_tool\n",
      "    result = self._invoke_tool_inner(node, f, kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 201, in _invoke_tool_inner\n",
      "    raise e\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 182, in _invoke_tool_inner\n",
      "    return f(**kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 576, in wrapper\n",
      "    raise WrappedOpenAIError(e)\n",
      "promptflow.tools.exception.WrappedOpenAIError: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "2024-06-04 14:50:51 +0000    3647 execution          WARNING  [llm_response in line 0 (index starts from 0)] stderr> Exception occurs: NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "2024-06-04 14:50:51 +0000    3647 execution          ERROR    Node llm_response in line 0 failed. Exception: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors].\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 543, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/aoai.py\", line 171, in chat\n",
      "    completion = self._client.chat.completions.create(**params)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_integrations/_openai_injector.py\", line 88, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 90, in invoke_tool\n",
      "    result = self._invoke_tool_inner(node, f, kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 201, in _invoke_tool_inner\n",
      "    raise e\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 182, in _invoke_tool_inner\n",
      "    return f(**kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 576, in wrapper\n",
      "    raise WrappedOpenAIError(e)\n",
      "promptflow.tools.exception.WrappedOpenAIError: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "2024-06-04 14:50:51 +0000    3627 execution          WARNING  [llm_response in line 1 (index starts from 0)] stderr> Exception occurs: NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "2024-06-04 14:50:51 +0000    3627 execution          ERROR    Node llm_response in line 1 failed. Exception: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors].\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 543, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/aoai.py\", line 171, in chat\n",
      "    completion = self._client.chat.completions.create(**params)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_integrations/_openai_injector.py\", line 88, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 90, in invoke_tool\n",
      "    result = self._invoke_tool_inner(node, f, kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 201, in _invoke_tool_inner\n",
      "    raise e\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 182, in _invoke_tool_inner\n",
      "    return f(**kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 576, in wrapper\n",
      "    raise WrappedOpenAIError(e)\n",
      "promptflow.tools.exception.WrappedOpenAIError: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "2024-06-04 14:50:51 +0000    3656 execution          WARNING  [llm_response in line 3 (index starts from 0)] stderr> Exception occurs: NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "2024-06-04 14:50:51 +0000    3656 execution          ERROR    Node llm_response in line 3 failed. Exception: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors].\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 543, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/aoai.py\", line 171, in chat\n",
      "    completion = self._client.chat.completions.create(**params)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_integrations/_openai_injector.py\", line 88, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 90, in invoke_tool\n",
      "    result = self._invoke_tool_inner(node, f, kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 201, in _invoke_tool_inner\n",
      "    raise e\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 182, in _invoke_tool_inner\n",
      "    return f(**kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 576, in wrapper\n",
      "    raise WrappedOpenAIError(e)\n",
      "promptflow.tools.exception.WrappedOpenAIError: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "2024-06-04 14:50:58 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:2)-Process id(3637)-Line number(2) completed.\n",
      "2024-06-04 14:50:58 +0000    3470 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2024-06-04 14:50:58 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:2)-Process id(3637)-Line number(4) start execution.\n",
      "2024-06-04 14:50:58 +0000    3470 execution.bulk     INFO     Average execution time for completed lines: 16.01 seconds. Estimated time for incomplete lines: 64.04 seconds.\n",
      "2024-06-04 14:50:59 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:1)-Process id(3627)-Line number(1) completed.\n",
      "2024-06-04 14:50:59 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:4)-Process id(3656)-Line number(3) completed.\n",
      "2024-06-04 14:50:59 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:3)-Process id(3647)-Line number(0) completed.\n",
      "2024-06-04 14:50:59 +0000    3470 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2024-06-04 14:50:59 +0000    3470 execution.bulk     INFO     Average execution time for completed lines: 4.27 seconds. Estimated time for incomplete lines: 4.27 seconds.\n",
      "2024-06-04 14:51:00 +0000    3637 execution          WARNING  [llm_response in line 4 (index starts from 0)] stderr> Exception occurs: NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "2024-06-04 14:51:00 +0000    3637 execution          ERROR    Node llm_response in line 4 failed. Exception: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors].\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 543, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/aoai.py\", line 171, in chat\n",
      "    completion = self._client.chat.completions.create(**params)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_integrations/_openai_injector.py\", line 88, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_utils/_utils.py\", line 277, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/resources/chat/completions.py\", line 590, in create\n",
      "    return self._post(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1240, in post\n",
      "    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 921, in request\n",
      "    return self._request(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/openai/_base_client.py\", line 1020, in _request\n",
      "    raise self._make_status_error_from_response(err.response) from None\n",
      "openai.NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'}\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 90, in invoke_tool\n",
      "    result = self._invoke_tool_inner(node, f, kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 201, in _invoke_tool_inner\n",
      "    raise e\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/_core/flow_execution_context.py\", line 182, in _invoke_tool_inner\n",
      "    return f(**kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 576, in wrapper\n",
      "    raise WrappedOpenAIError(e)\n",
      "promptflow.tools.exception.WrappedOpenAIError: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "2024-06-04 14:51:01 +0000    3470 execution.bulk     INFO     Process name(ForkProcess-8:2:2)-Process id(3637)-Line number(4) completed.\n",
      "2024-06-04 14:51:02 +0000    3470 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2024-06-04 14:51:02 +0000    3470 execution.bulk     INFO     Average execution time for completed lines: 3.82 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2024-06-04 14:51:02 +0000    3470 execution.bulk     INFO     The thread monitoring the process [3627-ForkProcess-8:2:1] will be terminated.\n",
      "2024-06-04 14:51:02 +0000    3470 execution.bulk     INFO     The thread monitoring the process [3637-ForkProcess-8:2:2] will be terminated.\n",
      "2024-06-04 14:51:02 +0000    3470 execution.bulk     INFO     The thread monitoring the process [3656-ForkProcess-8:2:4] will be terminated.\n",
      "2024-06-04 14:51:02 +0000    3470 execution.bulk     INFO     The thread monitoring the process [3647-ForkProcess-8:2:3] will be terminated.\n",
      "2024-06-04 14:51:02 +0000    3627 execution.bulk     INFO     The process [3627] has received a terminate signal.\n",
      "2024-06-04 14:51:02 +0000    3637 execution.bulk     INFO     The process [3637] has received a terminate signal.\n",
      "2024-06-04 14:51:02 +0000    3656 execution.bulk     INFO     The process [3656] has received a terminate signal.\n",
      "2024-06-04 14:51:02 +0000    3647 execution.bulk     INFO     The process [3647] has received a terminate signal.\n",
      "2024-06-04 14:51:03 +0000    3470 execution          ERROR    5/5 flow run failed, indexes: [0,1,2,3,4], exception of index 0: OpenAI API hits NotFoundError: Error code: 404 - {'error': 'Model meta-llama/Meta-Llama-3-8B-Instruct//openai/deployments/meta_llama3_instruct_Az/chat/completions does not exist'} [Error reference: https://platform.openai.com/docs/guides/error-codes/api-errors]\n",
      "2024-06-04 14:51:03 +0000    3470 promptflow-runtime INFO     Post processing batch result...\n",
      "2024-06-04 14:51:06 +0000    3470 execution.bulk     INFO     Upload status summary metrics for run 2024_06_04_145016chat_base_run finished in 3.0960950670014427 seconds\n",
      "2024-06-04 14:51:06 +0000    3470 promptflow-runtime INFO     Successfully write run properties {\"_azureml.evaluate_artifacts\": \"[{\\\"path\\\": \\\"instance_results.jsonl\\\", \\\"type\\\": \\\"table\\\"}]\", \"azureml.promptflow.total_tokens\": 0, \"azureml.promptflow.completion_tokens\": 0, \"azureml.promptflow.prompt_tokens\": 0} with run id '2024_06_04_145016chat_base_run'\n",
      "2024-06-04 14:51:06 +0000    3470 execution.bulk     INFO     Upload RH properties for run 2024_06_04_145016chat_base_run finished in 0.08281472000089707 seconds\n",
      "2024-06-04 14:51:06 +0000    3470 promptflow-runtime INFO     Creating Artifact for Run 2024_06_04_145016chat_base_run...\n",
      "2024-06-04 14:51:06 +0000    3470 promptflow-runtime INFO     Created instance_results.jsonl Artifact.\n",
      "2024-06-04 14:51:06 +0000    3470 promptflow-runtime WARNING  [2024_06_04_145016chat_base_run] Run failed. Execution stackTrace: Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/common.py\", line 543, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tools/aoai.py\", line 171, in chat\n",
      "    completion = self._client.chat.completions.create(**params)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_integrations/_openai_injector.py\", line 88, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/tracing/_trace.py\", line 513, in wrapped\n",
      "    output = func(*args, **kwargs)\n",
      "  [REDACTED: External StackTrace]\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/executor/flow_executor.py\", line 1027, in _exec\n",
      "    output, aggregation_inputs = self._exec_inner_with_trace(\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/executor/flow_executor.py\", line 932, in _exec_inner_with_trace\n",
      "    output, nodes_outputs = self._traverse_nodes(inputs, context)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/executor/flow_executor.py\", line 1208, in _traverse_nodes\n",
      "    nodes_outputs, bypassed_nodes = self._submit_to_scheduler(context, inputs, batch_nodes)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/executor/flow_executor.py\", line 1263, in _submit_to_scheduler\n",
      "    return scheduler.execute(self._line_timeout_sec)\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/executor/_flow_nodes_scheduler.py\", line 131, in execute\n",
      "    raise e\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/executor/_flow_nodes_scheduler.py\", line 113, in execute\n",
      "    self._dag_manager.complete_nodes(self._collect_outputs(completed_futures))\n",
      "  File \"/azureml-envs/prompt-flow/runtime/lib/python3.9/site-packages/promptflow/executor/_flow_nodes_scheduler.py\", line 160, in _collect_outputs\n",
      "    each_node_result = each_future.result()\n",
      "  [REDACTED: External StackTrace]\n",
      "\n",
      "2024-06-04 14:51:06 +0000    3470 promptflow-runtime INFO     Ending the aml run '2024_06_04_145016chat_base_run' with status 'Completed'...\n",
      "======= Run Summary =======\n",
      "Run name: \"2024_06_04_145016chat_base_run\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2024-06-04 14:50:41.757782+00:00\"\n",
      "Duration: \"0:00:25.050132\"\n",
      "Run url: \"https://ai.azure.com/projectflows/trace/run/2024_06_04_145016chat_base_run/details?wsid=/subscriptions/28e0cd19-9f05-4b6c-bac8-3bb37e8eeee3/resourcegroups/rg-deployment6/providers/Microsoft.MachineLearningServices/workspaces/contoso-chat-sf-aiproj\""
     ]
    },
    {
     "data": {
      "text/plain": [
       "<promptflow._sdk.entities._run.Run at 0x79c7ff7ddd90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pf_azure_client_hf.stream(base_run_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_azure_client.stream(base_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.customerId</th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>inputs.chat_history</th>\n",
       "      <th>outputs.answer</th>\n",
       "      <th>outputs.context</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outputs.line_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>tell me about your hiking jackets</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey Sarah Lee! üåü Let me tell you about our awe...</td>\n",
       "      <td>[{'id': '17', 'title': 'RainGuard Hiking Jacke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you have any climbing gear?</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey John! üëã Absolutely, we have some great cli...</td>\n",
       "      <td>[{'id': '9', 'title': 'SummitClimber Backpack'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Can you tell me about your selection of tents?</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey Michael! üëã We have a great selection of te...</td>\n",
       "      <td>[{'id': '15', 'title': 'SkyView 2-Person Tent'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Do you have any hiking boots?</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey Emily Rodriguez! üëã Absolutely, we have som...</td>\n",
       "      <td>[{'id': '4', 'title': 'TrekReady Hiking Boots'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What gear do you recommend for hiking?</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "      <td>Hey Jane! üå≤üèïÔ∏è For hiking, I recommend the Trai...</td>\n",
       "      <td>[{'id': '10', 'title': 'TrailBlaze Hiking Pant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    inputs.customerId  \\\n",
       "outputs.line_number                     \n",
       "0                                   4   \n",
       "1                                   1   \n",
       "2                                   3   \n",
       "3                                   6   \n",
       "4                                   2   \n",
       "\n",
       "                                                    inputs.question  \\\n",
       "outputs.line_number                                                   \n",
       "0                                 tell me about your hiking jackets   \n",
       "1                                    Do you have any climbing gear?   \n",
       "2                    Can you tell me about your selection of tents?   \n",
       "3                                     Do you have any hiking boots?   \n",
       "4                            What gear do you recommend for hiking?   \n",
       "\n",
       "                     inputs.line_number inputs.chat_history  \\\n",
       "outputs.line_number                                           \n",
       "0                                     0                  []   \n",
       "1                                     1                  []   \n",
       "2                                     2                  []   \n",
       "3                                     3                  []   \n",
       "4                                     4                  []   \n",
       "\n",
       "                                                        outputs.answer  \\\n",
       "outputs.line_number                                                      \n",
       "0                    Hey Sarah Lee! üåü Let me tell you about our awe...   \n",
       "1                    Hey John! üëã Absolutely, we have some great cli...   \n",
       "2                    Hey Michael! üëã We have a great selection of te...   \n",
       "3                    Hey Emily Rodriguez! üëã Absolutely, we have som...   \n",
       "4                    Hey Jane! üå≤üèïÔ∏è For hiking, I recommend the Trai...   \n",
       "\n",
       "                                                       outputs.context  \n",
       "outputs.line_number                                                     \n",
       "0                    [{'id': '17', 'title': 'RainGuard Hiking Jacke...  \n",
       "1                    [{'id': '9', 'title': 'SummitClimber Backpack'...  \n",
       "2                    [{'id': '15', 'title': 'SkyView 2-Person Tent'...  \n",
       "3                    [{'id': '4', 'title': 'TrekReady Hiking Boots'...  \n",
       "4                    [{'id': '10', 'title': 'TrailBlaze Hiking Pant...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf_azure_client.get_details(base_run)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputs.customerId</th>\n",
       "      <th>inputs.question</th>\n",
       "      <th>inputs.line_number</th>\n",
       "      <th>inputs.chat_history</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outputs.line_number</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>tell me about your hiking jackets</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Do you have any climbing gear?</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Can you tell me about your selection of tents?</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>Do you have any hiking boots?</td>\n",
       "      <td>3</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>What gear do you recommend for hiking?</td>\n",
       "      <td>4</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    inputs.customerId  \\\n",
       "outputs.line_number                     \n",
       "0                                   4   \n",
       "1                                   1   \n",
       "2                                   3   \n",
       "3                                   6   \n",
       "4                                   2   \n",
       "\n",
       "                                                    inputs.question  \\\n",
       "outputs.line_number                                                   \n",
       "0                                 tell me about your hiking jackets   \n",
       "1                                    Do you have any climbing gear?   \n",
       "2                    Can you tell me about your selection of tents?   \n",
       "3                                     Do you have any hiking boots?   \n",
       "4                            What gear do you recommend for hiking?   \n",
       "\n",
       "                     inputs.line_number inputs.chat_history  \n",
       "outputs.line_number                                          \n",
       "0                                     0                  []  \n",
       "1                                     1                  []  \n",
       "2                                     2                  []  \n",
       "3                                     3                  []  \n",
       "4                                     4                  []  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details = pf_azure_client_hf.get_details(base_run_hf)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Eval run on Json Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_flow = \"multi_flow/\"\n",
    "\n",
    "data = \"../data/salestestdata.jsonl\"\n",
    "run_name = timestamp+\"chat_eval_run\"\n",
    "print(run_name)\n",
    "\n",
    "run_name_hf = timestamp_hf+\"chat_eval_run\"\n",
    "print(run_name_hf)\n",
    "\n",
    "eval_run_variant = pf_azure_client.run(\n",
    "    flow=eval_flow,\n",
    "    data=data,  # path to the data file\n",
    "    run=base_run,  # use run as the variant\n",
    "    column_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${data.customerId}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"context\": \"${run.outputs.context}\",\n",
    "        # reference the run's output\n",
    "        \"answer\": \"${run.outputs.answer}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    "    display_name=run_name,\n",
    "    name=run_name\n",
    ")\n",
    "\n",
    "eval_run_variant_hf = pf_azure_client_hf.run(\n",
    "    flow=eval_flow,\n",
    "    data=data,  # path to the data file\n",
    "    run=base_run_hf,  # use run as the variant\n",
    "    column_mapping={\n",
    "        # reference data\n",
    "        \"customerId\": \"${data.customerId}\",\n",
    "        \"question\": \"${data.question}\",\n",
    "        \"context\": \"${run.outputs.context}\",\n",
    "        # reference the run's output\n",
    "        \"answer\": \"${run.outputs.answer}\",\n",
    "    },\n",
    "    runtime=runtime,\n",
    "    display_name=run_name_hf,\n",
    "    name=run_name_hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_azure_client.stream(eval_run_variant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_azure_client_hf.stream(eval_run_variant_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf_azure_client.get_details(eval_run_variant)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details = pf_azure_client_hf.get_details(eval_run_variant_hf)\n",
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metrics = pf_azure_client.get_metrics(eval_run_variant)\n",
    "print(json.dumps(metrics, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_hf = pf_azure_client.get_metrics(eval_run_variant_hf)\n",
    "print(json.dumps(metrics_hf, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_azure_client.visualize([base_run, eval_run_variant])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
